
1
 Dynamical Distance Learning for Unsupervised andSemi-Supervised Skill DiscoveryKristian Hartikainen†hartikainen@berkeley.eduXinyang Geng†young.geng@berkeley.eduTuomas Haarnoja∗†§tuomash@google.comSergey Levine∗†svlevine@eecs.berkeley.eduAbstract:Reinforcement learning requires manual specification of a reward func-tion to learn a task. While in principle this reward function only needs to specifythe task goal, in practice reinforcement learning can be very time-consuming oreven infeasible unless the reward function is shaped so as to provide a smooth gra-dient towards a successful outcome.  This shaping is difficult to specify by hand,particularly when the task is learned from raw observations, such as images.  Inthis paper, we study how we can automatically learn dynamical distances: a mea-sure of the expected number of time steps to reach a given goal state from anyother state. These dynamical distances can be used to provide well-shaped rewardfunctions for reaching new goals, making it possible to learn complex tasks effi-ciently.  We also show that dynamical distances can be used in a semi-supervisedregime, where unsupervised interaction with the environment is used to learn thedynamical distances, while a small amount of preference supervision is used todetermine the task goal, without any manually engineered reward function or goalexamples.  We evaluate our method both in simulation and on a real-world robot.We show that our method can learn locomotion skills in simulation without anysupervision.  We also show that it can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and ten preference labels, without anyother supervision.  Videos of the learned skills can be found on the project web-site:https://sites.google.com/view/skills-via-distance-learningKeywords:Deep Reinforcement Learning, Semi-Supervised Learning1  IntroductionThe manual design of reward functions represents a major barrier to the adoption of reinforcementlearning (RL), particularly in robotics, where vision-based policies can be learned end-to-end [1, 2],but  still  require  reward  functions  that  themselves  might  need  visual  detectors  to  be  designed  byhand [3].  While in principle the reward only needs to specify the goal of the task, in practice RLcan be exceptionally time-consuming or even infeasible unless the reward function is shaped so asto provide a smooth gradient towards a successful outcome. Prior work tackles such situations withdedicated exploration methods [4, 5, 6], or by using large amounts of random exploration [7], whichis feasible in simulation but infeasible for real-world robotic learning. In robotics, it is also commonto employ heuristic shaping, such as the Cartesian distance to a goal for an object relocation task [8,9].  However,  this kind of shaping is brittle and requires manual insight, and is often impossiblewhen Cartesian positions are unavailable, such as when learning from image observations.In this paper, we aim to address these challenges by introducing dynamical distance learning (DDL),a general method for learning distance functions that can provide effective shaping for goal-reachingtasks without manual engineering.  Instead of imposing heuristic metrics that have no relationshipto the system dynamics, we quantify the distance between two states in terms of the number of time†UC Berkeley,§Google DeepMind,∗Equal advising. Preprint. Under Review.arXiv:1907.08225v1  [cs.LG]  18 Jul 2019
Figure 1:We present a dynamical distance learning (DDL) method that can learn a 9-DoF real-world dexterousmanipulation  task  directly  from  raw  image  observations.   DDL  does  not  assume  access  to  the  true  rewardfunction  and  solves  the  180  degree  valve-rotation  task  in  9  hours  by  relying  only  on  20  operator-providedpreference labels.steps needed to transition between them.  This is a natural choice for dynamical systems, and priorworks have explored learning such distances in simple and low-dimensional domains [10].  Whilesuch distances can be learned using standard model-free reinforcement learning algorithms, such asQ-learning, we show that such methods generally struggle to acquire meaningful distances for morecomplex systems, particularly with high-dimensional observations such as images.  We present asimple method that employs supervised regression to fit dynamical distances, and then uses thesedistances to provide reward shaping, guide exploration, and discover distinct skills.We can use dynamical distances to alleviate the need for hand-specified reward functions in a varietyof applications.  The most direct use of DDL is to provide reward shaping for a standard deep RLalgorithm, to optimize a policy to reach a given goal state. We can also use DDL in a fully unsuper-vised method, where the most distant states are selected for exploration, resulting in an unsupervisedreinforcement learning procedure that discovers difficult skills that reach dynamically distant statesfrom a given start state.  Finally, we can formulate a semi-supervised skill learning method, wherea user expresses preferences over goals, and the agent autonomously collects experience to learndynamical distances in a self-supervised way.  All of these applications avoid the need for manu-ally designed reward functions, demonstrations, or user-provided examples, and involve minimalmodification to existing deep RL algorithms.The main contribution of our work is a method called dynamical distance learning (DDL), whichlearns to predict the expected number of time steps needed for a policy to reach one state fromanother state,  and which can be used for reward shaping,  unsupervised skill learning,  and semi-supervised skill discovery from preferences.  DDL is a simple and scalable approach to learningdynamical distances that can readily accommodate raw image inputs and, as shown in our exper-iments,  substantially outperforms prior methods that learn goal-conditioned policies or distancesusing approximate dynamic programming techniques, such as Q-learning.  We show that using dy-namical distances as a reward function in standard reinforcement learning methods results in policiesthat take the shortest path to a given goal, despite the additional shaping.  Empirically, we compareour method to prior methods for unsupervised skill discovery on tasks ranging from 2D navigationto quadrupedal locomotion.  We also compare the semi-supervised variant of our method to priortechniques for learning from preferences.  Our experimental evaluation demonstrates that DDL canlearn complex locomotion skills without any supervision at all, and that the preferences-based ver-sion of DDL can learn to turn a valve with a real-world 9-DoF hand, using raw image observationsand 20 preference labels, without any other supervision.2  Related WorkDDL is most closely related to methods that learn goal-conditioned policies or value functions [11,12].  Many of these works learn goal-reaching directly via model-free RL, often by using temporaldifference updates to learn the distance function as a value function [13, 11, 6, 14, 15, 16].  Forexample, Kaelbling [10] learns a goal conditioned Q-function to represent the shortest path betweenany two states, and Andrychowicz et al. [6] learns a value function that resembles a distance to goals,under  a  user-specified  low-dimensional  goal  representation.   Unlike  these  methods,  DDL  learnspolicy-conditioned distances with an explicit supervised learning procedure, and then employs thesedistances to recover a reward function for RL. We experimentally compare to RL-based distancelearning methods, and show that DDL attains substantially better results, especially with complexobservations. Another line of prior work uses a learned distance to build a search graph over a set ofvisited states [17, 18], which can then be used to plan to reach new states via the shortest path. Ourmethod also learns a distance function separately from the policy, but instead of using it to build agraph, we use it to obtain a reward function for a separate model-free RL algorithm.2

1
 Dynamics-Aware Unsupervised Discovery of SkillsArchit Sharma∗, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol HausmanGoogle Brain{architsh, shanegu, slevine, vikashplus, karolhausman}@google.comAbstractConventionally, model-based reinforcement learning (MBRL) aims to learn a globalmodel for the dynamics of the environment. A good model can potentially enableplanning algorithms to generate a large variety of behaviors and solve diverse tasks.However, learning an accurate model for complex dynamical systems is difficult,and even then, the model might not generalize well outside the distribution of stateson which it was trained.  In this work, we combine model-based learning withmodel-free learning of primitives that make model-based planning easy. To thatend, we aim to answer the question: how can we discover skills whose outcomes areeasy to predict? We propose an unsupervised learning algorithm, Dynamics-AwareDiscovery of Skills (DADS), which simultaneously discoverspredictablebehaviorsand learns their dynamics. Our method can leverage continuous skill spaces, theo-retically, allowing us to learn infinitely many behaviors even for high-dimensionalstate-spaces. We demonstrate thatzero-shot planningin the learned latent spacesignificantly outperforms standard MBRL and model-free goal-conditioned RL,can handle sparse-reward tasks, and substantially improves over prior hierarchicalRL methods for unsupervised skill discovery. Video demonstration of our resultsare available at:https://sites.google.com/view/dads-skillFigure 1:  A humanoid agent discovers diverse locomotion primitiveswithout any rewardusing DADS. Weshow zero-shot generalization to downstream tasks by composing the learned primitives using model predictivecontrol, enabling the agent to follow an online sequence of goals (green markers) without any additional training.1  IntroductionDeep reinforcement learning (RL) enables autonomous learning of diverse and complex tasks withrich sensory inputs, temporally extended goals, and challenging dynamics, such as discrete game-playing domains   [46,62],  and continuous control domains including locomotion [59,31] andmanipulation [56,34,25].  Most of the deep RL approaches learn a Q-function or a policy thatare directly optimized for the training task, which limits their generalization to new scenarios.  Incontrast, MBRL methods [45,15,72] can acquire dynamics models that may be utilized to performunseen tasks at test time. While this capability has been demonstrated in some of the recent works∗Work done as a member of Google AI Residency Program (g.co/airesidency).Preprint. Under review.arXiv:1907.01657v1  [cs.LG]  2 Jul 2019
[44,49,12,41,26], learning an accurate global model that works for all state-action pairs can beexceedingly challenging, especially for high-dimensional system with complex and discontinuousdynamics. The problem is further exacerbated as the learned global model has limited generalizationoutside of the state distribution it was trained on and exploring the whole state space is generallyinfeasible. Can we retain the flexibility of model-based RL, while using model-free RL to acquireproficient low-level behaviors under complex dynamics?While learning a global dynamics model that captures all the different behaviors for the entire state-space can be extremely challenging, learning a model for a specific behavior that acts only in a smallpart of the state-space can be much easier. For example, consider learning a model for dynamics ofall gaits of a quadruped versus a model which only works for a specific gait. If we can learn manysuch behaviors and their corresponding dynamics, we can leverage model-predictive control to planin thebehavior space, as opposed to planning in the action space. The question then becomes: howdo we acquire such behaviors, considering that behaviors could be random and unpredictable? Tothis end, we proposeDynamics-Aware Discovery of Skills(DADS), an unsupervised RL frameworkfor learning low-level skills using model-free RL with the explicit aim of making model-basedcontrol easy. Skills obtained using DADS are directly optimized forpredictability, providing a betterrepresentation on top of which predictive models can be learned. Crucially, the skills do not requireany supervision to learn, and are acquired entirely through autonomous exploration. This means thatthe repertoire of skills and their predictive model are learned before the agent has been tasked withany goal or reward function. When a task is provided at test-time, the agent utilizes the previouslylearned skills and model to immediately perform the task without any further training.The key contribution of our work is an unsupervised reinforcement learning algorithm,  DADS,grounded in mutual-information-based exploration. We demonstrate that our objective can embedlearned primitives in continuous spaces,  which allows us to learn a large,  diverse set of skills.Crucially, our algorithm also learns to model the dynamics of the skills, which enables the use ofmodel-based planning algorithms for downstream tasks. We adapt the conventional model predictivecontrol algorithms to plan in the space of primitives, and demonstrate that we can compose thelearned primitives to solve downstream tasks without any additional training.2  PreliminariesMutual information has been used as an objective to encourage exploration in reinforcement learning[32,47]. According to its definition,I(A;B) =H(A)−H(A|B), optimizing mutual informationIwith respect toBamounts to maximizing the entropyHofAwhile minimizing the conditionalentropyH(A|B). IfAis a function of the state andBrepresents actions, this objective encouragesthe state entropy to be high, causing the underlying policy to be exploratory. Recently, multiple works[17, 24, 2] apply this idea to learn diverse skills which maximally cover the state space.To leverage planning-based control,  MBRL estimates the true dynamics of the environment bylearning a modelˆp(s′|s,a). This allows it to predict a trajectory of statesˆτH= (st,ˆst+1,...ˆst+H)resulting from a sequence of actions without any additional interaction with the environment.  Asimilar simulation of the trajectoryˆτHcan be carried out using a model parameterized asq(s′|s,z),wherezdenotes the skill that is being executed. This modification to MBRL not only mandates theexistence of a policyπ(·|s,z)executing the actual actions in environment, but more importantly, thepolicy to execute these actions in a way that maintainspredictability underq. In this setup, skillszareeffectively an abstraction for the actionsa1,a2...that are executed in the environment. This schemeforgoes a much harder task of learning a global modelˆp, in exchange of a collection of potentiallysimpler models of behavior-specific dynamics. In addition, the planning problem becomes easier asthe planner is searching over a skill spacezthat can act on longer horizons than granular actionsa.These seemingly unrelated ideas can be combined into a single optimization scheme, where we firstdiscover skills (and their models) without any extrinsic reward and then compose these skills tooptimize for the task defined at test time using model-based planning.  At train time, we assumea Markov Decision Process (MDP)M1≡(S,A,p).  The state spaceSand action spaceAareassumed to be continuous, and theAbounded. We assume the transition dynamicspto be stochastic,such thatp:S×A×S 7→[0,∞). We learn a skill-conditioned policyπ(a|s,z), where the skillszbelongs to the spaceZ, detailed in Section 3. We assume that the skills are sampled from a priorp(z)overZ. We simultaneously learn a skill-conditioned transition functionq(s′|s,z), coined as2

1
 Stochastic Latent Actor-Critic:Deep Reinforcement Learningwith a Latent Variable ModelAlex X. LeeAnusha NagabandiPieter AbbeelSergey LevineUniversity of California, Berkeley{alexlee_gk,nagaban2,pabbeel,svlevine}@cs.berkeley.eduAbstractDeep reinforcement learning (RL) algorithms can use high-capacity deep networksto learn directly from image observations.  However, these kinds of observationspaces present a number of challenges in practice, since the policy must now solvetwo problems:  a representation learning problem, and a task learning problem.In this paper, we aim to explicitly learn representations that can accelerate re-inforcement learning from images.  We propose the stochastic latent actor-critic(SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learn-ing policies for complex continuous control tasks directly from high-dimensionalimage inputs. SLAC learns a compact latent representation space using a stochas-tic sequential latent variable model, and then learns a critic model within thislatent space.  By learning a critic within a compact state space, SLAC can learnmuch more efficiently than standard RL methods. The proposed model improvesperformance substantially over alternative representations as well, such as vari-ational autoencoders. In fact, our experimental evaluation demonstrates that thesample efficiency of our resulting method is comparable to that of model-basedRL methods that directly use a similar type of model for control. Furthermore, ourmethod outperforms both model-free and model-based alternatives in terms of finalperformance and sample efficiency, on a range of difficult image-based controltasks. Our code and videos of our results are available at our website.11    IntroductionDeep reinforcement learning (RL) algorithms can automatically learn to solve certain tasks from raw,low-level observations such as images. However, these kinds of observation spaces present a numberof challenges in practice: on one hand, it is difficult to directly learn from these high-dimensionalinputs, but on the other hand, it is also difficult to tease out a compact representation of the underlyingtask-relevant information from which to learn instead.  For these reasons, deep RL directly fromlow-level observations such as images remains a challenging problem. Particularly in continuousdomains governed by complex dynamics, such as robotic control [32,3], standard approaches stillrequire separate sensor setups to monitor details of interest in the environment, such as the jointpositions of a robot or specific pose information of objects of interest. To instead be able to learndirectly from the more general and rich modality of vision would greatly advance the current state ofour learning systems, so we aim to study precisely this. Standard model-free deep RL aims to usedirect end-to-end training to explicitly unify these tasks of representation learning and task learning.However, solving both problems together is difficult, since an effective policy requires an effectiverepresentation, but in order for an effective representation to emerge, the policy or value functionPreprint. Under review.1https://alexlee-gk.github.io/slac/arXiv:1907.00953v1  [cs.LG]  1 Jul 2019
must provide meaningful gradient information using only the model-free supervision signal (i.e.,the reward function).  In practice, learning directly from images with standard RL algorithms canbe slow, sensitive to hyperparameters, and inefficient. In contrast to end-to-end learning with RL,predictive learning can benefit from a rich and informative supervision signal before the agent haseven made progress on the task or received any rewards. This leads us to ask: can we explicitlylearna latent representation from raw low-level observations that makes deep RL easier, through learning apredictive latent variable model?Predictive models are commonly used in model-based RL for the purpose of planning [7,9,28,6,34]or generating cheap synthetic experience for RL to reduce the required amount of interaction with thereal environment [31,12]. However, in this work, we are primarily concerned with their potential toalleviate therepresentation learningchallenge in RL. We devise a stochastic predictive model bymodeling the high-dimensional observations as the consequence of a latent process, with a Gaussianprior and latent dynamics, as illustrated in Figure 1. A model with an entirely stochastic latent statehas the appealing interpretation of being able to properly represent uncertainty about any of the statevariables, given its past observations. We demonstrate in our work that fully stochastic state spacemodels can in fact be learned effectively:  With a well-designed stochastic network, such modelsoutperform fully deterministic models, and contrary to the observations in prior work [16,4], areactually comparable to (if not better than) mixed deterministic/stochastic models. Finally, we notethat this explicit representation learning, even on low-reward data, allows an agent with such a modelto make progress on representation learning even before it makes progress on task learning.Equipped with this model, we can then perform RLin the learned latent spaceof the predictivemodel.  We posit—and confirm experimentally—that our latent variable model provides a usefulrepresentation for RL. Our model represents a partially observed Markov decision process (POMDP),and solving such a POMDP exactly would be computationally intractable [1,21,20]. We insteadpropose a simple approximation that trains a Markovian critic on the (stochastic) latent state andtrains an actor on a history of observations and actions. The resulting stochastic latent actor-critic(SLAC) algorithm loses some of the benefits of full POMDP solvers, but it is easy and stable to train.It also produces good results, in practice, on a range of challenging problems, making it an appealingalternative to more complex POMDP solution methods.The main contributions of our SLAC algorithm are useful representations learned from our stochasticsequential latent variable model, as well as effective RL in this learned latent space.  We showexperimentally that our approach substantially improves onbothmodel-free and model-based RLalgorithms on a range of image-based continuous control benchmark tasks, attaining better finalperformance and learning more quickly than algorithms based on (a) end-to-end deep RL fromimages, (b) learning in a latent space produced by various alternative latent variable models, such asa variational autoencoder (VAE) [24], and (c) model-based RL based on latent state-space modelswith mixed deterministic/stochastic variables [16].2    Related WorkRepresentation learning in RL.End-to-end deep RL can in principle learn representations directlyas part of the RL process [27].  However, prior work has observed that RL has a “representationlearning bottleneck”:  a considerable portion of the learning period must be spent acquiring goodrepresentations of the observation space [30].  This motivates the use of a distinct representationlearning procedure to acquire these representations before the agent has even learned to solve thetask. The use of unsupervised learning to learn such representations has been explored in a numberof prior works [25,10]. In contrast to this class of representation learning algorithms where temporalconnections between data are not explicitly considered, we utilize a latent-space dynamics model,which we find in empirical comparisons to result in substantially better representations for RL.By modeling covariances between consecutive latent states, we make it feasible for our proposedstochastic latent actor-critic (SLAC) algorithm to perform Bellman backups directly in the latentspace of the learned model. In contrast to prior work that also uses latent-space dynamical systemmodels [33,22,34,16], our approach benefits from the good asymptotic performance of model-freeRL, while at the same time leveraging the improved latent space representation for sample efficiency,despite not using any model-based rollouts for data augmentation.2

1
 When to Trust Your Model:Model-Based Policy OptimizationMichael Janner    Justin Fu    Marvin Zhang    Sergey LevineUniversity of California, Berkeley{janner, justinjfu, marvin, svlevine}@eecs.berkeley.eduAbstractDesigning effective model-based reinforcement learning algorithms is difficultbecause the ease of data generation must be weighed against the bias of model-generated data.  In this paper, we study the role of model usage in policy opti-mization both theoretically and empirically.  We first formulate and analyze amodel-based reinforcement learning algorithm with a guarantee of monotonic im-provement at each step. In practice, this analysis is overly pessimistic and suggeststhat real off-policy data is always preferable to model-generated on-policy data,but we show that an empirical estimate of model generalization can be incorpo-rated into such analysis to justify model usage.  Motivated by this analysis, wethen demonstrate that a simple procedure of using short model-generated rolloutsbranched from real data has the benefits of more complicated model-based algo-rithms without the usual pitfalls. In particular, this approach surpasses the sampleefficiency of prior model-based methods, matches the asymptotic performance ofthe best model-free algorithms, and scales to horizons that cause other model-basedmethods to fail entirely.1  IntroductionReinforcement learning algorithms generally fall into one of two categories: model-based approaches,which build a predictive model of an environment and derive a controller from it, and model-freetechniques, which learn a direct mapping from states to actions. Model-free methods have shownpromise as a general-purpose tool for learning complex policies from raw state inputs (Mnih et al.,2015; Lillicrap et al., 2016; Haarnoja et al., 2018), but their generality comes at the cost of efficiency.When dealing with real-world physical systems, for which data collection can be an arduous process,model-based approaches are appealing due to their comparatively fast learning.  However, modelaccuracy acts as a bottleneck to policy quality, often causing model-based approaches to performworse asymptotically than their model-free counterparts.In this paper,  we study how to most effectively use a predictive model for policy optimization.We first formulate and analyze  a class of model-based  reinforcement learning algorithms withimprovement guarantees.  Although there has been recent interest in monotonic improvement ofmodel-based reinforcement learning algorithms (Sun et al., 2018; Luo et al., 2019), most commonlyused model-based approaches lack the improvement guarantees that underpin many model-freemethods (Schulman et al., 2015). While it is possible to apply analogous techniques to the study ofmodel-based methods to achieve similar guarantees, it is more difficult to use such analysis to justifymodel usage in the first place due to pessimistic bounds on model error. However, we show that morerealistic model error rates derived empirically allow us to modify this analysis to provide a morereasonable tradeoff on model usage.Our main contribution is a practical algorithm built on these insights, which we call model-basedpolicy optimization (MBPO), that makes limited use of a predictive model to achieve pronouncedPreprint. Under review.arXiv:1906.08253v1  [cs.LG]  19 Jun 2019
improvements in performance compared to other model-based approaches. More specifically, wedisentangle the task horizon and model horizon by querying the model only for short rollouts. Weempirically demonstrate that a large amount of these short model-generated rollouts can allow apolicy optimization algorithm to learn substantially faster than recent model-based alternatives whileretaining the asymptotic performance of the most competitive model-free algorithms. We also showthat MBPO does not suffer from the same pitfalls as prior model-based approaches, avoiding modelexploitation and failure on long-horizon tasks. Finally, we empirically investigate different strategiesfor model usage, supporting the conclusion that careful use of short model-based rollouts providesthe most benefit to a reinforcement learning algorithm.2  Related WorkModel-based reinforcement learning methods are promising candidates for real-world sequentialdecision-making problems due to their data efficiency (Kaelbling et al., 1996). For example, Gaussianprocesses and time-varying linear dynamical systems provide excellent performance in the low-dataregime (Deisenroth & Rasmussen, 2011; Levine & Koltun, 2013; Kumar et al., 2016). Neural networkpredictive models, both deterministic (Draeger et al., 1995; Nagabandi et al., 2018) and probabilistic(Gal et al., 2016; Depeweg et al., 2016), are appealing because they may allow for algorithms thatcombine the sample efficiency of a model-based approach with the asymptotic performance ofhigh-capacity function approximators, even in domains with complex observations such as images(Oh et al., 2015; Ebert et al., 2018; Kaiser et al., 2019). Our work uses an ensemble of probabilisticnetworks, as in Chua et al. (2018), although our model is employed to learn a policy rather than inthe context of a receding-horizon planning routine.Learned models may be incorporated into otherwise model-free methods for improvements in dataefficiency. For example, a model-free policy can be used as an action proposal distribution within amodel-based planner (Piché et al., 2019). Conversely, model rollouts may be used to provide extratraining examples for a Q-function (Sutton, 1990), to improve the target value estimates of existingdata points (Feinberg et al., 2018), or to provide additional context to a policy (Du & Narasimhan,2019). However, the performance of such approaches rapidly degrades with increasing model error(Gu et al., 2016), motivating work that interpolates between different rollout lengths (Buckman et al.,2018), tunes the ratio of real to model-generated data (Kalweit & Boedecker, 2017), or does not relyon model predictions (Heess et al., 2015). Our approach similarly tunes model usage during policyoptimization, but we show that justifying non-negligible model usage during most points in trainingrequires consideration of the model’s ability to generalize outside of its training distribution.Prior methods have also explored incorporating computation that resembles model-based planningbut without constraining the intermediate predictions of the planner to match plausible environmentobservations (Tamar et al., 2016; Racanière et al., 2017; Oh et al., 2017; Silver et al., 2017). Whilesuch methods can reach asymptotic performance on par with model-free approaches, they may notbenefit from the sample efficiency of model-based methods as they forgo the extra supervision usedin standard model-based methods.The bottleneck in scaling model-based approaches to complex tasks often lies in learning accuratepredictive models of high-dimensional dynamics (Atkeson & Schaal, 1997). Model ensembles haveshown to be effective in preventing a policy or planning procedure from exploiting the inaccuraciesof any single model (Rajeswaran et al., 2017; Kurutach et al., 2018; Clavera et al., 2018; Chua et al.,2018). Alternatively, a model may also be trained on its own outputs to avoid compounding errorfrom multi-step predictions (Talvitie, 2014, 2016) or predict many timesteps into the future (Whitney& Fergus, 2018).  We empirically demonstrate that a combination of model ensembles with shortmodel rollouts is sufficient to prevent exploitation of model inaccuracies.Theoretical analysis of model-based reinforcement learning algorithms has been considered by Sunet al. (2018) and Luo et al. (2019), who bound the discrepancy between returns under a model andthose in the real environment of interest. Their approaches enforce a trust region around a referencepolicy, whereas we do not constrain the policy but instead consider rollout length based on estimatedmodel generalization capacity. Alternate analyses have been carried out by incorporating the structureof the value function into the model learning (Farahmand et al., 2017) or by regularizing the modelby controlling its Lipschitz constant (Asadi et al., 2018). Prior work has also constructed complexitybounds for model-based approaches in the tabular setting (Szita & Szepesvari, 2010) and for thelinear quadratic regulator (Dean et al., 2017), whereas we consider general non-linear systems.2

1
 Deep  Reinforcement  Learning  for  Industrial  Insertion  Taskswith  Visual  Inputs  and  Natural  RewardsGerrit Schoettler∗1, Ashvin Nair∗2, Jianlan Luo2, Shikhar Bahl2,Juan Aparicio Ojea1, Eugen Solowjow1, Sergey Levine2Abstract— Connector  insertion  and  many  other  tasks  com-monly found in modern manufacturing settings involve complexcontact  dynamics  and  friction.  Since  it  is  difficult  to  capturerelated  physical  effects  with  first-order  modeling,  traditionalcontrol  methods  often  result  in  brittle  and  inaccurate  con-trollers,   which   have   to   be   manually   tuned.   Reinforcementlearning  (RL)  methods  have  been  demonstrated  to  be  capableof learning controllers in such environments from autonomousinteraction  with  the  environment,  but  running  RL  algorithmsin the real world poses sample efficiency and safety challenges.Moreover,  in  practical  real-world  settings  we  cannot  assumeaccess  to  perfect  state  information  or  dense  reward  signals.In  this  paper,  we  consider  a  variety  of  difficult  industrialinsertion tasks with visual inputs and different natural rewardspecifications, namely sparse rewards and goal images. We showthat methods that combine RL with prior information, such asclassical  controllers  or  demonstrations,  can  solve  these  tasksfrom  a  reasonable  amount  of  real-world  interaction.I.  INTRODUCTIONMany industrial tasks on the edge of automation require adegree of adaptability that is difficult to achieve with conven-tional robotic automation techniques. While standard controlmethods,  such  as  PID  controllers,  are  heavily  employed  toautomate  many  tasks  in  the  context  of  positioning,  tasksthat require significant adaptability or tight visual perception-control loops are often beyond the capabilities of such meth-ods,  and  therefore  are  typically  performed  manually.  Stan-dard control methods can struggle in presence of complex dy-namical phenomena that are hard to model analytically, suchas  complex  contacts.  Reinforcement  learning  (RL)  offers  adifferent solution, relying on trial and error learning insteadof  accurate  modeling  to  construct  an  effective  controller.RL  with  expressive  function  approximation,  i.e.  deep  RL,has further shown to automatically handle high dimensionalinputs such as images [1].However, deep RL has thus far not seen wide adoption inthe automation community due to several practical obstacles.Sample efficiency is one obstacle: tasks must be completedwithout  excessive  interaction  time  or  wear  and  tear  on  therobot.  Progress  in  recent  years  on  developing  better  RLalgorithms  has  led  to  significantly  better  sample  efficiency,even in dynamically complicated tasks [2], [3], but remains achallenge for deploying RL in real-world robotics contexts.Another major, often underappreciated, obstacle is goal spec-ification: while prior work in RL assumes a reward signal tooptimize, it is often carefully shaped to allow the system to∗First two authors contributed equally,1Siemens Corporation,2University of California, Berkeley.Fig. 1: We train an agent directly in the real world to solveconnector  insertion  tasks  from  raw  pixel  input  and  withoutaccess to ground-truth state information for reward functions.Left, a rollout from a learned policy that successfully com-pletes the insertion task for each connector is shown. Right,a full view of the robot setup including the three connectorswe  use  in  this  work.  Videos  of  the  results  are  available  atindustrial-insertion-rl.github.iolearn [4], [5], [6]. Obtaining such dense reward signals canbe  a  significant  challenge,  as  one  must  additionally  build  aperception  system  that  allows  computing  dense  rewards  onstate  representations.  Shaping  a  reward  function  so  that  anagent can learn from it is also a manual process that requiresconsiderable manual effort. An ideal RL system would learnfrom rewards that are natural and easy to specify. How canwe  enable  robots  to  autonomously  perform  complex  taskswithout  significant  engineering  effort  to  design  perceptionand reward systems?We  first  consider  an  end-to-end  approach  that  learns  apolicy  from  images,  where  the  images  serve  as  both  thestate  representation  and  the  goal  specification.  Using  goalimages  is  not  fully  general,  but  can  successfully  representtasks  when  the  task  is  to  reach  a  final  desired  state  [7].Specifying goals via goal images is convenient, and makes itpossible to specify goals with minimal manual effort. Usingimages as the state representation also allows a robot to learnbehaviors that utilize direct visual feedback, which providessome robustness to sensor and actuator noise.Secondly,  we  consider  learning  from  simple  and  sparsereward signals. Sparse rewards can often be obtained conve-niently,  for  instance  from  human-provided  labels  or  simpleinstrumentation.  In  many  electronic  assembly  tasks,  whichwe  consider  here,  we  can  directly  detect  whether  the  elec-tronics are functional, and use that signal as a reward. Learn-ing  from  sparse  rewards  poses  a  challenge,  as  explorationwith sparse reward signals is difficult, but by using sufficientprior  information  about  the  task,  one  can  overcome  thisarXiv:1906.05841v1  [cs.RO]  13 Jun 2019
Fig.  2:  A  close-up  view  of  the  three  connector  insertiontasks  shows  the  contacts  and  tight  tolerances  the  agentmust navigate to solve these tasks. These tasks require sub-millimeter precision without visual feedback.challenge.  To  handle  this  challenge,  we  extend  the  residualRL  approach  [8],  [9],  which  learns  a  parametric  policy  ontop  of  a  fixed,  hand-specified  controller,  to  the  setting  ofvision-based manipulation.In  our  experiments,  we  show  that  we  can  successfullycomplete  real-world  tight  tolerance  assembly  tasks,  suchas  inserting  USB  connectors,  using  RL  from  images  withreward  signals  that  are  convenient  for  users  to  specify.  Weshow  that  we  can  learn  from  only  a  sparse  reward  basedon  the  electrical  connection  for  a  USB  adapter  plug,  andwe demonstrate learning insertion skills with rewards basedonly on goal images. These reward signals require no extraengineering and are easy to specify for many tasks. Beyondshowing  the  feasibility  of  RL  for  solving  these  tasks,  weevaluate multiple RL algorithms across three tasks and studytheir robustness to imprecise positioning and noise.II.  RELATEDWORKLearning  has  been  applied  previously  in  a  variety  ofrobotics  contexts.  Different  forms  of  learning  have  enabledautonomous   driving   [10],   biped   locomotion   [11],   blockstacking   [12],   grasping   [13],   and   navigation   [14],   [15].Among these methods, many involve reinforcement learning,where  an  agent  learns  to  perform  a  task  by  maximizinga  reward  signal.  Reinforcement  learning  algorithms  havebeen developed and applied to teach robots to perform taskssuch  as  balancing  a  robot  [16],  playing  ping-pong  [17]and baseball [18]. The use of large function approximators,such  as  neural  networks,  in  RL  has  further  broadened  thegenerality  of  RL  [1].  Such  techniques,  called  “deep”  RL,have further allowed robots to be trained directly in the realworld to perform fine-grained manipulation tasks from vision[19], open doors [20], play  hockey [21], stack Lego blocks[22], use dexterous hands [23], and grasp objects [24]. In thiswork  we  further  explore  solving  real-world  robotics  tasksusing RL.Many  RL  algorithms  introduce  prior  information  aboutthe  specific  task  to  be  solved.  One  common  method  isreward shaping [4], but reward shaping can become arbitrar-ily  difficult  as  the  complexity  of  the  task  increases.  Othermethods incorporate a trajectory planner [25], [26], [27] butfor complex assembly tasks, trajectory planners require a hostof  information  about  objects  and  geometries  which  can  bedifficult to provide.Another body of work on incorporating prior informationstudies  using  demonstrations  either  to  initialize  a  policy[18], [28], infer reward functions using inverse reinforcementlearning [29], [30], [31], [32], [33] or to improve the policythroughout  the  learning  procedure  [34],  [35],  [36],  [37].These  methods  require  multiple  demonstrations,  which  canbe difficult to collect, especially for assembly tasks, althoughlearning  a  reward  function  by  classifying  goal  states  [38],[39]  may  partially  alleviate  this  issue.  More  recently,  man-ually  specifying  a  policy  and  learning  the  residual  task  hasbeen proposed [8], [9]. In this work we evaluate both residualRL and combining RL with learning from demonstrations.Previous  work  has  also  tackled  high  precision  assemblytasks,  especially  insertion-type  tasks.  One  line  of  work  fo-cuses on obtaining high dimensional observations, includinggeometry,  forces,  joint  positions  and  velocities  [40],  [41],[42],  [43],  but  this  information  is  not  easily  procured,  in-creasing complexity of the experiments and the supervisionrequired. Other work relies on external trajectory planning orvery high precision control [42], [41], but this can be brittleto error in other components of the system, such as percep-tion.  We  show  how  our  method  not  only  solves  insertiontasks with much less information about the environment, butalso does so under noisy conditions.III.  ELECTRICCONNECTORPLUGINSERTIONTASKSIn  this  work,  we  empirically  evaluate  learning  methodson  a  set  of  electric  connector  assembly  tasks,  pictured  inFig. 2. Connector plug insertions are difficult for two reasons.First,  the  robot  must  be  very  precise  in  lining  up  the  plugwith  its  socket.  As  we  show  in  our  experiments,  errors  assmall as±1mm can lead to consistent failure. Second, thereis  significant  friction  when  the  connector  plug  touches  thesocket,  and  the  robot  must  learn  to  apply  sufficient  forcein  order  to  insert  the  plug.  Image  sequences  of  successfulinsertions are shown in Fig. 1, where it is also possible to seedetails of the gripper setup that we used to ensure a failurefree,  fully  automated  training  process.  In  our  experiments,we  use  a  7  degrees  of  freedom  Sawyer  robot  with  end-effector  control,  meaning  that  the  action  signalutcan  beinterpreted as the relative end-effector movement in Cartesiancoordinates. The robot’s underlying internal control pipelineis illustrated in Fig. 3.Fig.  3:  Illustration  of  the  robot’s  cascade  control  scheme.The actionsutare computed at a frequency of up to10 Hz,desired joint angles are obtained by inverse kinematics, anda  joint-space  impedance  controller  with  anti-windup  PIDcontrol commands actuator torques at1000 Hz.

1
 Efficient Exploration via State Marginal MatchingLisa Lee∗Carnegie Mellon Universitylslee@cs.cmu.eduBenjamin Eysenbach∗CMU, Google Brainbeysenba@cs.cmu.eduEmilio Parisotto∗Carnegie Mellon Universityeparisot@cs.cmu.eduEric XingCarnegie Mellon UniversitySergey LevineUC Berkeley, Google BrainRuslan SalakhutdinovCarnegie Mellon UniversityAbstractTo solve tasks with sparse rewards, reinforcement learning algorithms must beequipped with suitable exploration techniques. However, it is unclear what under-lying objective is being optimized by existing exploration algorithms, or how theycan be altered to incorporate prior knowledge about the task.  Most importantly,it is difficult to use exploration experience from one task to acquire explorationstrategies for another task.  We address these shortcomings by learning a singleexploration policy that can quickly solve a suite of downstream tasks in a multi-tasksetting, amortizing the cost oflearning to explore.  We recast exploration as aproblem ofState Marginal Matching (SMM): we learn a mixture of policies forwhich the state marginal distribution matches a given target state distribution, whichcan incorporate prior knowledge about the task. Without any prior knowledge, theSMM objective reduces to maximizing the marginal state entropy. We optimize theobjective by reducing it to a two-player, zero-sum game, where we iteratively fit astate density model and then update the policy to visit states with low density underthis model.  While many previous algorithms for exploration employ a similarprocedure, they omit a crucialhistorical averagingstep, without which the iterativeprocedure does not converge to a Nash equilibria. To parallelize exploration, weextend our algorithm to use mixtures of policies, wherein we discover connectionsbetween SMM and previously-proposed skill learning methods based on mutualinformation. On complex navigation and manipulation tasks, we demonstrate thatour algorithm explores faster and adapts more quickly to new tasks.11    IntroductionIn order to solve tasks with sparse or delayed rewards, reinforcement learning (RL) algorithms mustbe equipped with suitable exploration techniques.  Exploration methods based on random actionshave limited ability to cover a wide range of states. More sophisticated techniques, such as intrinsicmotivation, can be much more effective. However, it is often unclear what underlying objective isoptimized by these methods, or how prior knowledge can be readily incorporated into the explorationstrategy.  Most importantly, it is difficult to use exploration experience from one task to acquireexploration strategies for another task.We address these shortcomings by considering a multi-task setting, where many different rewardfunctions can be provided for the same set of states and dynamics. Rather than re-inventing the wheeland learning to explore anew for each task, we aim to learn asingle, task-agnostic exploration policy∗Equal Contribution.1Videos and code:https://sites.google.com/view/state-marginal-matchingarXiv:1906.05274v1  [cs.LG]  12 Jun 2019
that can be adapted to many possible downstream reward functions, amortizing the cost of learning toexplore. This exploration policy can be viewed as a prior on the policy for solving downstream tasks.Learning will consist of two phases: during training, we acquire this task-agnostic exploration policy;during testing, we use this exploration policy to quickly explore and maximize the task reward.Learning a single exploration policy is considerably more difficult than doing exploration throughoutthe course of learning a single task. The latter is done by intrinsic motivation [Oudeyer et al., 2007,Pathak et al., 2017, Tang et al., 2017] and count-based exploration methods [Bellemare et al., 2016],which can effectively explore to find states with high reward, at which point the agent can decreaseexploration and increase exploitation of those high-reward states.  While these methods performefficient exploration for learning a single task, the policy at any particular iteration is not a goodexploration policy. For example, the final policy at convergence would only visit the high-rewardstates discovered for the current task.  A straightforward solution is to simply take the historicalaverage over policies from each iteration of training. At test time, we can sample one of the historicalpolicies from a previous training iteration, and use the corresponding policy to sample actions in thatepisode. Our algorithm will implicitly do this.What objective should be optimized during training to obtain a good exploration policy?We recastexploration as a problem of State Marginal Matching: given a desired state distribution, we learn amixture of policies for which the state marginal distribution matches this desired distribution. Withoutany prior information, this objective reduces to maximizing the marginal state entropyH[s], whichencourages the policy to visit as many states as possible. The distribution matching objective alsoprovides a convenient mechanism for humans to incorporate prior knowledge about the task, whetherin the form of constraints that the agent should obey; preferences for some states over other states;reward shaping; or the relative importance of each state dimension for a particular task.We propose an algorithm to optimize the State Marginal Matching (SMM) objective. First, we reducethe problem of SMM to a two-player, zero-sum game between a policy player and a density player.We find a Nash Equilibrium for this game usingfictitious play[Brown, 1951], a classic procedurefrom game theory.  Our resulting algorithm iteratively fits a state density model and then updatesthe policy to visit states with low density under this model.  While many previous algorithms forexploration employ a similar procedure, they omit a crucialhistorical averagingstep, without whichthe iterative procedure is not guaranteed to converge.In short,  our paper studies the State Marginal Matching objective as a principled objective foracquiring a task-agnostic exploration policy. We propose an algorithm to optimize this objective. Ouranalysis of this algorithm sheds light on prior methods, and we empirically show that SMM solveshard exploration tasks faster than state-of-the-art baselines in navigation and manipulation domains.2    Related WorkMost prior work on exploration has looked at exploration bonuses and intrinsic motivation. Typically,these algorithms [Burda et al., 2018, Houthooft et al., 2016, Oudeyer et al., 2007, Pathak et al.,2017, Schmidhuber, 1991] formulate some auxiliary task, and use prediction error on that task asan exploration bonus. Another class of methods [Bellemare et al., 2016, Schmidhuber, 2010, Tanget al., 2017] directly encourage the agent to visit novel states. While all methods effectively exploreduring the course of solving a single task, the policy obtained at convergence is often not a goodexploration policy. For example, consider an exploration bonus derived from prediction error of aninverse model [Pathak et al., 2017]. At convergence, the inverse model will have high error at stateswith stochastic dynamics, so the resulting policy will always move towards these stochastic statesand fail to explore the rest of the state space.Many  exploration  algorithms  can  be  classified  by  whether  they  do  exploration  in  the  space  ofactions, policy parameters, goals, or states. Common exploration strategies including-greedy andOrnstein–Uhlenbeck noise [Lillicrap et al., 2015], as well as standard MaxEnt algorithms [Haarnojaet al., 2018, Ziebart, 2010], do exploration in action space.  Recent work [Fortunato et al., 2017,Plappert et al., 2017] shows that adding noise to the parameters of the policy can result in goodexploration. Most closely related to our work are methods [Hazan et al., 2018, Pong et al., 2019]that perform exploration in the space of states or goals.  In fact, Hazan et al. [2018], consider thesame State Marginal Matching objective that we examine. However, the algorithm proposed thererequires an oracle planner and an oracle density model, assumptions that our method will not require.2

1
 Search on the Replay Buffer:Bridging Planning and Reinforcement LearningBenjamin Eysenbachθφ, Ruslan Salakhutdinovθ, Sergey LevineφψθCMU,φGoogle Brain,ψUC Berkeleybeysenba@cs.cmu.eduAbstractThe history of learning for control has been an exciting back and forth betweentwo broad classes of algorithms: planning and reinforcement learning. Planningalgorithms effectively reason over long horizons, but assume access to a localpolicy and distance metric over collision-free paths. Reinforcement learning excelsat learning policies and the relative values of states, but fails to plan over longhorizons.  Despite the successes of each method in various domains, tasks thatrequire reasoning over long horizons with limited feedback and high-dimensionalobservations remain exceedingly challenging for both planning and reinforcementlearning algorithms.  Frustratingly, these sorts of tasks are potentially the mostuseful, as they are simple to design (a human only need to provide an examplegoal state) and avoid reward shaping, which can bias the agent towards findinga sub-optimal solution.  We introduce a general-purpose control algorithm thatcombines the strengths of planning and reinforcement learning to effectively solvethese tasks.  Our aim is to decompose the task of reaching a distant goal stateinto a sequence of easier tasks, each of which corresponds to reaching a particularsubgoal. Planning algorithms can automatically find these waypoints, but only ifprovided with suitable abstractions of the environment – namely, a graph consistingof nodes and edges.  Our main insight is that this graph can be constructed viareinforcement learning, where a goal-conditioned value function provides edgeweights, and nodes are taken to be previously seen observations in a replay buffer.Using graph search over our replay buffer, we can automatically generate thissequence of subgoals, even in image-based environments. Our algorithm, searchon the replay buffer (SoRB), enables agents to solve sparse reward tasks over onehundred steps, and generalizes substantially better than standard RL algorithms.11    IntroductionHow can agents learn to solve complex, temporally extended tasks? Classically, planning algorithmsgive us one tool for learning such tasks.  While planning algorithms work well for tasks whereit is easy to determine distances between states and easy to design a local policy to reach nearbystates, both of these requirements become roadblocks when applying planning to high-dimensional(e.g., image-based) tasks.  Learning algorithms excel at handling high-dimensional observations,but reinforcement learning (RL) – learning for control – fails to reason over long horizons to solvetemporally extended tasks. In this paper, we propose a method that combines the strengths of planningand RL, resulting in an algorithm that can plan over long horizons in tasks with high-dimensionalobservations.Recent work has introduced goal-conditioned RL algorithms (Pong et al., 2018; Schaul et al., 2015)that acquire a single policy for reaching many goals. In practice, goal-conditioned RL succeeds at1Run our algorithm in your browser:http://bit.ly/rl_searcharXiv:1906.05253v1  [cs.AI]  12 Jun 2019
Figure 1:Search on the Replay Buffer:(a) Goal-conditioned RL often fails to reach distant goals,but can successfully reach the goal if starting nearby (inside the green region).  (b) Our goal is touse observations in our replay buffer (yellow squares) as waypoints leading to the goal.  (c) Weautomatically find these waypoints by using the agent’s value function to predict when two statesare nearby, and building the corresponding graph. (d) We run graph search to find the sequence ofwaypoints (blue arrows), and then use our goal-conditioned policy to reach each waypoint.reaching nearby goals but fails to reach distant goals; performance degrades quickly as the number ofsteps to the goal increases (Levy et al., 2019; Nachum et al., 2018). Moreover, goal-conditioned RLoften requires large amounts of reward shaping (Chiang et al., 2019) or human demonstrations (Lynchet al., 2019; Nair et al., 2018), both of which can limit the asymptotic performance of the policy bydiscouraging the policy from seeking novel solutions.We propose to solve long-horizon, sparse reward tasks by decomposing the task into a series of easiergoal-reaching tasks. We learn a goal-conditioned policy for solving each of the goal-reaching tasks.Our main idea is to reduce the problem of finding these subgoals to solving a shortest path problemover states that we have previous visited, using a distance metric extracted from our goal-conditionedpolicy. We call this algorithm Search on Replay Buffer (SoRB), and provide a simple illustration ofthe algorithm in Figure 1.Our primary contribution is an algorithm that bridges planning and deep RL for solving long-horizon,sparse reward tasks.   We develop a practical instantiation of this algorithm using ensembles ofdistributional value functions, which allows us torobustlylearn distances and use them forrisk-awareplanning.  Empirically, we find that our method generates effective plans to solve long horizonnavigation tasks, even in image-based domains, without a map and without odometry. Comparisonswith state-of-the-art RL methods show that SoRB is substantially more successful in reaching distantgoals. We also observe that the learned policy generalizes well to navigate in unseen environments.In summary, graph search over previously visited states is a simple tool for boosting the performanceof a goal-conditioned RL algorithm.2    Bridging Planning and Reinforcement LearningPlanning algorithms must be able to (1) sample valid states,  (2) estimate the distance betweenreachable  pairs  of  states,  and  (3)  use  a  local  policy  to  navigate  between  nearby  states.   Theserequirements are difficult to satisfy in complex tasks with high dimensional observations, such asimages. For example, consider a robot arm stacking blocks using image observations. Sampling statesrequires generating photo-realistic images, and estimating distances and choosing actions requiresreasoning about dozens of interactions between blocks. Our method will obtain distance estimatesand a local policy using a RL algorithm.  To sample states, we will simply use a replay buffer ofpreviously visited states as a non-parametric generative model.2.1    Building Block: Goal-Conditioned RLA key building block of our method is a goal-conditioned policy and its associated value function.We consider a goal-reaching agent interacting with an environment. The agent observes its currentstates∈ Sand a goal statesg∈ S.  The initial state for each episode is sampleds1∼ρ(s), anddynamics are governed by the distributionp(st+1|st,at). At every step, the agent samples an actiona∼π(a|s,sg)and receives a corresponding rewardr(s,a,sg)that indicates whether the agenthas reached the goal. The episode terminates as soon as the agent reaches the goal, or afterTsteps,whichever occurs first. The agent’s task is to maximize its cumulative,undiscounted, reward. We usean off-policy algorithm to learn such a policy, as well as its associated goal-conditioned Q-function2

1
 Learning Powerful Policies by Using Consistent Dynamics ModelShagun Sodhani1Anirudh Goyal1Tristan Deleu1Yoshua Bengio1 2Sergey Levine3Jian Tang1AbstractModel-based Reinforcement Learning approacheshave the promise of being sample efficient. Muchof the progress in learning dynamics models inRL has been made  by learning models via su-pervised learning.  But traditional model-basedapproaches lead to “compounding errors” whenthe model is unrolled step by step.  Essentially,the state transitions that the learner predicts (byunrolling the model for multiple steps) and thestate transitions that the learner experiences (byacting in the environment) may not beconsistent.There is enough evidence that humans build amodel of the environment, not only by observingthe environment but also by interacting with theenvironment.  Interaction with the environmentallows humans to carry outexperiments: takingactions that help uncover true causal relationshipswhich can be used for building better dynamicsmodels. Analogously, we would expect such in-teractions to be helpful for a learning agent whilelearning to model the environment dynamics. Inthis paper, we build upon this intuition by usingan auxiliary cost function to ensure consistencybetween what the agent observes (by acting inthe real world) and what it imagines (by acting inthe “learned” world). We consider several tasks -Mujoco based control tasks and Atari games - andshow that the proposed approach helps to trainpowerful policies and better dynamics models.1. IntroductionReinforcement Learning consists of two fundamental prob-lems:learningandplanning.Learningcomprises of im-proving the agent’s current policy by interacting with theenvironment whileplanninginvolves improving the policywithout interacting with the environment. These problems1Mila,   University   of   Montreal2CIFAR   Senior   Fellow3University of California, Berkeley. Correspondence to: ShagunShodhani<sshagunsodhani@gmail.com>.Accepted at RLDM 2019, The Multi-disciplinary Conference onReinforcement Learning and Decision Making. Copyright 2019by the author(s).evolve into the dichotomy ofmodel-freemethods (which pri-marily rely onlearning) andmodel-basedmethods (whichprimarily rely onplanning).  Recently,model-freemeth-ods have shown many successes, such as learning to playAtari games with pixel observations (Mnih et al., 2015b;Mnih et al., 2016) and learning complex motion skills fromhigh dimensional inputs (Schulman et al., 2015a;b).  Buttheir high sample complexity is still a major criticism of themodel-freeapproaches.In contrast, model-based reinforcement learning methodshave been introduced in the literature where the goal isto improve the sample efficiency by learning a dynamicsmodel of the environment. But model-based RL has severalcaveats.  If the policy takes the learner to an unexploredstate in the environment, the learner’s model could makeerrors in estimating the environment dynamics, leading tosub-optimal behavior.  This problem is referred to as themodel-bias problem (Deisenroth & Rasmussen, 2011).In order to make a prediction about the future, dynamicsmodels are unrolled step by step which leads to “compound-ing errors” (Talvitie, 2014; Bengio et al., 2015; Lamb et al.,2016): an error in modeling the environment at timetaffectsthe predicted observations at all subsequent time-steps. Thisproblem is much more challenging for the environmentswhere the agent observes high-dimensional image inputs(and not compact state representations). On the other hand,model-free algorithms are not limited by the accuracy of themodel, and therefore can achieve better final performance bytrial and error, though at the expense of much higher samplecomplexity. In the model-based approaches, the dynamicsmodel is usually trained with supervised learning techniquesand the state transition tuples (collected as the agent actsin the environment) become the supervising dataset. Hencethe process of learning the model has no control over whatkind of data is produced for its training. That is, from theperspective of learning the dynamics model, the agent justobserves the environment and does not “interact” with it.On the other hand, there’s enough evidence that humanslearn the environment dynamics not just by observing theenvironment but also by interacting with the environment(Cook et al., 2011; Daniels & Nemenman, 2015). Interac-tion is useful as it allows the agent to carry out experimentsin the real world to determine causality, which is clearly adesirable characteristic when building dynamics models.arXiv:1906.04355v1  [cs.LG]  11 Jun 2019
Learning Powerful Policies by Using Consistent Dynamics ModelThis leads to an interesting possibility.  The agent couldconsider two possible pathways:  (i) Interacting with theenvironment by taking actions in the real world to generatenew observations and (ii) Interacting with the learned dy-namics models by imagining to take actions and predictingthe new observations.  Consider the humanoid robot fromthe MuJoCo environment (Mordatch et al., 2015).  In thefirst case, the humanoid agent takes an action in the realenvironment, observes the change in its position (and loca-tion), takes another step and so on. In the second case, theagent imagines taking a step, predicts what the observationwould look like, imagines taking another step and so on.The first case is theclosed-loopsetup, where the humanoidobserves the state of the world, takes an action, gets the trueobservation from the environment, which it uses to choosethe next action, and so on. The second case is theopen-loopsetup, where the agent predicts subsequent states for multi-ple time steps into the future (with the help of the dynamicsmodel) without interacting with the environment (see figure1).As such, the two pathways may not beconsistentgiven thechallenges in learning a multi-step dynamics model. Bycon-sistent, we mean the behavior of state transitions along thetwo paths should be indistinguishable. Had the predictionsfrom the open loop been similar to the predictions fromthe closed loop over a long time horizon, the two pathwayswould beconsistentand we could say that the learner’sdynamics model is grounded in reality.  To that end, ourcontributions are the following:1.We propose to ensure consistency by using an auxiliaryloss which explicitly matches the generative behavior(from the open loop) and the observed behavior (fromthe closed loop) as closely as possible.2.We show that the proposed approach helps to simulta-neously train more powerful policies as well as betterdynamics models, by using a training objective that isnot solely focused on predicting the next observation.3.We consider various tasks - 7 Mujoco based continu-ous control tasks and 4 Atari games - from OpenAIGym suite (Brockman et al., 2016), and RLLab (Duanet al., 2016) and show that using the proposed auxiliaryloss consistently helps in achieving better performanceacross all the tasks.4.We compare our proposed approach to the state-of-the-art state space models (Buesing et al., 2018) and showthat the proposed method outperforms the sophisticatedbaselines despite being very straightforward.We  also  evaluate  our  approach  on  the  pixel-based  Half-Cheetah  task  from  the  OpenAI  Gym  suite  (Brockmanet   al.,   2016).The   task   is   difficult   for   the   “base-line”  state-space  models  as  only  the  position  (and  notthe  velocity)  can  be  inferred  from  the  images,   mak-ing  the  task  partially  observable.    Our  implementationof  the  paper  is  available  athttps://github.com/shagunsodhani/consistent-dynamics.2. PrelimariesA  finite  time  Markov  decision  processMis  generallydefined  by  the  tuple(S,A,f,R,γ).   Here,Sis  the  setof  states,Athe  action  space,f(st+1|st,at)the  transi-tion  distribution,r:S × A →Ris  the  reward  func-tion  andγthe  discount  factor.   We  define  the  return  asthe discounted sum of rewardsr(st,at)along a trajectoryτ:= (s0,a0,...,sT−1,aT−1,sT), whereTrefers to the ef-fective horizon of the process.  The goal of reinforcementlearning is to find a policyπφthat maximizes the expectedreturn. Hereφdenotes the parameters of the policyπ.Model-based RL methods learn the dynamics model fromthe  observed  transitions.    This  is  usually  done  with  afunction approximator parameterized as a neural networkˆfθ(st+1|st,at). In such a case, the parametersθof the dy-namics model are optimized to maximize the log-likelihoodof the state transition distribution.3. Environment ModelConsider a learning agent training to optimize an expectedreturns signal in a given environment. At a given timestept,the agent is in some statest∈S. It takes an actionat∈Aaccording to its policyat∼πt(at|st), receives a rewardrt(from the environment) and transitions to a new statest+1.The agent is trying to maximize its expected returns and hastwo pathways for improving its behaviour:1.Closed-looppath:The learning agent interacts withthe environment by taking actions in the real worldat every step.  The agent starts in states0and is instatestat timet.  It chooses an actionatto perform(using its policyπt), performs the chosen action, andreceives a rewardrt. It then observes the environmentto obtain the new statest+1, uses this state to decidewhich actionat+1to perform next and so on.2.Open-looppath:The learning agent interacts with thelearned dynamics model byimaginingto take actionsand predicts the future observations (or future beliefstate in case of state space models). The agent starts instates0and is in statestat timet. Note that the agent“imagines” itself to be in statesItand can not access thetrue state of the environment. It chooses an actionatto perform (using its policyπt), performs the action inthe “learner’s” world (dynamics model) and imagines

1
 Watch, Try, Learn:Meta-Learning from Demonstrations and RewardsAllan Zhou1∗, Eric Jang1, Daniel Kappler2, Alex Herzog2, Mohi Khansari2,Paul Wohlhart2,Yunfei Bai2,Mrinal Kalakrishnan2,Sergey Levine1,3,Chelsea Finn11Google Brain, Mountain View, USA2X, Mountain View, USA3University of California Berkeley, Berkeley, USA{allanz,ejang,slevine,chelseaf}@google.com{kappler,alexherzog,khansari,wohlhart,yunfeibai,kalakris}@x.teamAbstractImitation learning allows agents to learn complex behaviors from demonstrations.However, learning a complex vision-based task may require an impractical numberof demonstrations. Meta-imitation learning is a promising approach towards en-abling agents to learn a new task from one or a few demonstrations by leveragingexperience from learning similar tasks. In the presence of task ambiguity or unob-served dynamics, demonstrations alone may not provide enough information; anagent must also try the task to successfully infer a policy. In this work, we proposea method that can learn to learn from both demonstrations and trial-and-error expe-rience with sparse reward feedback. In comparison to meta-imitation, this approachenables the agent to effectively and efficiently improve itself autonomously beyondthe demonstration data.  In comparison to meta-reinforcement learning, we canscale to substantially broader distributions of tasks, as the demonstration reducesthe burden of exploration.  Our experiments show that our method significantlyoutperforms prior approaches on a set of challenging, vision-based control tasks.1  IntroductionImitation learning enables autonomous agents to learn complex behaviors from demonstrations,which are often easy and intuitive for users to provide. However, learning expressive neural networkpolicies from imitation requires a large number of demonstrations, particularly when learning fromhigh-dimensional inputs such as image pixels. Meta-imitation learning has emerged as a promisingapproach for allowing an agent to leverage data from previous tasks in order to learn a new taskfrom only a handful of demonstrations [6,9,18].  However, in many practical few-shot imitationsettings, there is an identifiability problem: it may not be possible to precisely determine a policy fromone or a few demonstrations, especially in a new situation. And even if a demonstration preciselycommunicateswhatthe task entails, it might not precisely communicatehowto accomplish it in newsituations. For example, it may be difficult to discern from a single demonstration where to grasp anobject when it is in a new position or how much force to apply in order to slide an object withoutknocking it over. It may be expensive to collect more demonstrations to resolve such ambiguities,and even when we can, it may not be obvious to a human demonstrator where the agent’s difficulty∗Work done as a Google AI resident.Preprint. Under review.arXiv:1906.03352v1  [cs.LG]  7 Jun 2019
Figure 1: After watching one demonstration (left), the scene is re-arranged. With one trial episode(middle), our method can learn to solve the task in the retrial episode (right) by leveraging both thedemo and trial-and-error experience.is arising from.  Alternatively, it is easy for the user to provide success-or-failure feedback, whileexploratory interaction is useful for learning how to perform the task. To this end, our goal is to buildan agent that can first infer a policy from one demonstration, then attempt the task using that policywhile receiving binary user feedback, and finally use the feedback to improve its policy such that itcan consistently solve the task.This vision of learning new tasks from a few demonstrations and trials inherently requires someamount of prior knowledge or experience, which we can acquire through meta-learning across a rangeof previous tasks. To this end, we develop a new meta-learning algorithm that incorporates elements ofimitation learning with trial-and-error reinforcement learning. In contrast to previous meta-imitationlearning approaches that learn one-shot imitation learning procedures through imitation [6,9], ourapproach enables the agent to improve at the test task through trial-and-error.  Further, from theperspective of meta-RL algorithms that aim to learn efficient RL procedures [5,43,8], our approachalso has significant appeal:  as we aim to scale meta-RL towards broader task distributions andlearn increasingly general RL procedures, exploration and efficiency becomes exceedingly difficult.However, a demonstration can significantly narrow down the search space while also providing apractical means for a user to communicate the goal, enabling the agent to achieve few-shot learning ofbehavior. While the combination of demonstrations and reinforcement has been studied extensivelyin single task problems [21,39,30,23], this combination is particularly important in meta-learningcontexts where few-shot learning of new tasks is simply not possible without demonstrations. Further,we can even significantly improve upon prior methods that study this combination using meta-learningto more effectively integrate the information coming from both sources.The primary contribution of this paper is a meta-learning algorithm that enables effective learningof new behaviors with a single demonstration and trial experience. In particular, after receiving ademonstration that illustrates a new goal, the meta-trained agent can learn to accomplish that goalthrough trial-and-error with only binary success-or-failure labels.  We evaluate our algorithm andseveral prior methods on a challenging, vision-based control problem involving manipulation tasksfrom four distinct families of tasks: button-pressing, grasping, pushing, and pick and place. We findthat our approach can effectively learn tasks with new, held-out objects using one demonstrationand a single trial, while significantly outperforming meta-imitation learning, meta-reinforcementlearning, and prior methods that combine demonstrations and reward feedback. To our knowledge,our experiments are the first to show that meta-learning can enable an agent to adapt to new tasks withbinary reinforcement signals from raw pixel observations, which we show with a single meta-modelfor a variety of distinct manipulation tasks. Videos of our experimental results can be found on thesupplemental website2.2  Related WorkLearning to learn, or meta-learning, has a long-standing history in the machine learning literature [42,34,1,16].  We particularly focus on meta-learning in the context of control.  Our approach buildson and significantly improves upon meta-imitation learning [6,9,18,28] and meta-reinforcement2For video results, see website:https://sites.google.com/view/watch-try-learn-project2

1
 Off-Policy Evaluation via Off-Policy ClassificationAlex Irpan1, Kanishka Rao1, Konstantinos Bousmalis2,Chris Harris1,Julian Ibarz1,Sergey Levine1,31Google Brain, Mountain View, USA2DeepMind, London, UK3University of California Berkeley, Berkeley, USA{alexirpan,kanishkarao,konstantinos,ckharris,julianibarz,slevine}@google.comAbstractIn this work, we consider the problem of model selection for deep reinforcementlearning (RL) in real-world environments.  Typically, the performance of deepRL algorithms is evaluated via on-policy interactions with the target environment.However, comparing models in a real-world environment for the purposes of earlystopping or hyperparameter tuning is costly and often practically infeasible. Thisleads us to examine off-policy policy evaluation (OPE) in such settings. We focuson OPE for value-based methods, which are of particular interest in deep RL,with applications like robotics, where off-policy algorithms based on Q-functionestimation can often attain better sample complexity than direct policy optimization.Existing OPE metrics either rely on a model of the environment, or the use ofimportance sampling (IS) to correct for the data being off-policy.  However, forhigh-dimensional observations, such as images, models of the environment canbe difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper,we focus on the specific case of MDPs with continuous action spaces and sparsebinary rewards, which is representative of many important real-world applications.We propose an alternative metric that relies on neither models nor IS, by framingOPE  as  a  positive-unlabeled  (PU)  classification  problem  with  the  Q-functionas the decision function.  We experimentally show that this metric outperformsbaselines on a number of tasks. Most importantly, it can reliably predict the relativeperformance of different policies in a number of generalization scenarios, includingthe transfer to the real-world of policies trained in simulation for an image-basedrobotic manipulation task.1    IntroductionSupervised learning has seen significant advances in recent years, in part due to the use of large,standardized datasets [5]. When researchers can evaluate real performance of their methods on thesame data via a standardized offline metric, the progress of the field can be rapid.  Unfortunately,such metrics have been lacking in reinforcement learning (RL). Model selection and performanceevaluation in RL are typically done by estimating the average on-policy return of a method in thetarget environment. Although this is possible in most simulated environments [2,3,35], real-worldenvironments, like in robotics, make this difficult and expensive [34]. Off-policy evaluation (OPE)has the potential to change that: a robust off-policy metric could be used together with realistic andcomplex data to evaluate the expected performance of off-policy RL methods, which would enablePreprint. Under review.arXiv:1906.01624v2  [cs.LG]  20 Jun 2019
(a) Visual summary of off-policy metrics(b) Robotics grasping simulation-to-reality gap.Figure 1:(a) Visual illustration of our method:We propose using classification-based approachesto do off-policy evaluation. Solid curves representQ(s,a)over a positive and negative trajectory,with the dashed curve representingmaxa′Q(s,a′)along states the positive trajectory visits (thecorresponding negative curve is omitted for simplicity). Baseline approaches (blue, red) measure Q-function fit betweenQ(s,a)tomaxa′Q(s,a′). Our approach (purple) directly measures separationofQ(s,a)between positive and negative trajectories.(b) The visual “reality gap” of our mostchallenging task:off-policy evaluation of the generalization of image-based robotic agents trainedsolely in simulation (left) using historical data from the target real-world environment (right).rapid progress on important real-world RL problems. Furthermore, it would greatly simplify transferlearning in RL, where OPE would enable model selection and algorithm design in simple domains(e.g., simulation) while evaluating the performance of these models and algorithms on complexdomains (e.g., using previously collected real-world data).Previous approaches to off-policy evaluation [6,12,26,33] generally use importance sampling (IS)or learned dynamics models. However, this makes them difficult to use with many modern deep RLalgorithms. First, OPE is most useful in the off-policy RL setting, where we expect to use real-worlddata as the “validation set”, but many of the most commonly used off-policy RL methods are based onvalue function estimation, produce deterministic policies [19,36], and do not require any knowledgeof the policy that generated the real-world training data. This makes them difficult to use with IS.Furthermore, many of these methods might be used with high-dimensional observations, such asimages. Although there has been considerable progress in predicting future images [1,18], learningsufficiently accurate models in image space for effective evaluation is still an open research problem.We therefore aim to develop an OPE method that requires neither IS nor models.We observe that for model selection, it is sufficient to predict some statisticcorrelatedwith policyreturn, rather than directly predict policy return. We address the specific case of binary-reward MDPs:tasks where the agent receives a non-zero reward only once during an episode, at the final timestep(Sect. 2). These can be interpreted as tasks where the agent can either “succeed” or “fail” in each trial,and although they form a subset of all possible MDPs, this subset is quite representative of manyreal-world tasks, and is actively used e.g. in robotic manipulation [14,29]. The novel contributionof our method (Sect. 3) is to frame OPE as a positive-unlabeled (PU) classification [15] problem,which provides for a way to derive OPE metrics that are both(a)fundamentally different from priormethods based on IS and model learning, and(b)perform well in practice on both simulated andreal-world tasks.  Additionally, we identify and present (Sect. 4) a list of generalization scenariosin RL that we would want our metrics to be robust against. We experimentally show (Sect. 6) thatour suggested OPE metrics outperform a variety of baseline methods across all of the evaluationscenarios, including a simulation-to-reality transfer scenario for a vision-based robotic grasping task(see Fig. 1b).2    PreliminariesWe focus on finite–horizon, deterministic Markov decision processes (MDP). We define an MDP as(S,A,P,S0,r,γ).Sis the state–space,Athe action–space, and both can be continuous.Pdefinestransitions to next states given the current state and action,S0defines initial state distribution,risthe reward function, andγ∈[0,1]is the discount factor. Episodes are of finite lengthT: at a given2
